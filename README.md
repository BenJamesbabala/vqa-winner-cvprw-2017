# 2017 VQA Challenge Winner from CVPR'17
Pytorch implementation of [Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge by Teney et al](https://arxiv.org/pdf/1708.02711.pdf).

## Prerequisites
- Python 2.7+
- [NumPy](http://www.numpy.org/)
- [PyTorch](http://pytorch.org/)
- [tqdm](https://pypi.python.org/pypi/tqdm) (visualizing preprocessing progress only)
- [nltk](http://www.nltk.org/install.html) (also need [this](https://nlp.stanford.edu/software/tokenizer.shtml) to tokenize strings)
- *tqdm and nltk are not necessary if you directly use processed `vqa_train_final.json` in the `data/` directory*


## Data
- [VQA 2.0](http://visualqa.org/download.html)
- [COCO 36 features pretrained resnet model](https://github.com/peteanderson80/bottom-up-attention#pretrained-features)


## Preparation
## Train
## Notes
## Resources
- [The paper](https://arxiv.org/pdf/1708.02711.pdf).
- [Their CVPR Workshop slides](http://cs.adelaide.edu.au/~Damien/Research/VQA-Challenge-Slides-TeneyAnderson.pdf).

